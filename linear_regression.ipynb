{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressione Lineare con Spark MLlib\n",
    "\n",
    "### In questo notebook vedremo come eseguire una semplice regressione, utilizzando un modello di Regressione Lineare, con il modulo MLlib di Spark. Il modello che andremo a creare avrà lo scopo di stimare il valore di un'abitazione utilizzando un set di proprietà:\n",
    "\n",
    "#### CRIM Tasso di criminalità per capita\n",
    "#### ZN Percentuale di terreni residenziali suddivisi in zone per lotti superiori a 25.000 sq.ft.\n",
    "#### INDUS Percentuale di ettari di attività non al dettaglio per città.\n",
    "#### CHAS Variabile dummy che indica la prossimità al fiume Charles.\n",
    "#### NOX Concentrazione di ossido d'azoto (parti per 10 milioni).\n",
    "#### RM Numero medio di stanze per abitazione\n",
    "#### AGE Percentuale di abitazione occupate costruite dopo il 1940\n",
    "#### DIS Media pesata delle distanze da 5 centri lavorativi di Boston.\n",
    "#### RAD Indice di accessibilità ad autostrade\n",
    "#### TAX Aliquota dell'imposta sulla proprietà a valore pieno in 10.000 USD.\n",
    "#### PRATIO Rapporto studente-insegnante per città.\n",
    "#### BLACK 1000(Bk - 0.63)^2 dove Bk è la percentuale di abitanti di colore per città\n",
    "#### LSTAT Percentuale della popolazione povera\n",
    "#### MEDV Mediana del valore di abitazioni occupate in 1.000 USD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"regression\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+\n",
      "| ID|   crim|  zn|indus|chas|  nox|   rm| age|   dis|rad|tax|ptratio| black|lstat|medv|\n",
      "+---+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+\n",
      "|  1|0.00632|18.0| 2.31|   0|0.538|6.575|65.2|  4.09|  1|296|   15.3| 396.9| 4.98|24.0|\n",
      "|  2|0.02731| 0.0| 7.07|   0|0.469|6.421|78.9|4.9671|  2|242|   17.8| 396.9| 9.14|21.6|\n",
      "|  4|0.03237| 0.0| 2.18|   0|0.458|6.998|45.8|6.0622|  3|222|   18.7|394.63| 2.94|33.4|\n",
      "|  5|0.06905| 0.0| 2.18|   0|0.458|7.147|54.2|6.0622|  3|222|   18.7| 396.9| 5.33|36.2|\n",
      "|  7|0.08829|12.5| 7.87|   0|0.524|6.012|66.6|5.5605|  5|311|   15.2| 395.6|12.43|22.9|\n",
      "+---+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "housing_df = spark.read.csv(\"./data/houses_train.csv\", inferSchema=True, header=True)\n",
    "housing_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing dei dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'black', 'lstat']\n"
     ]
    }
   ],
   "source": [
    "features_cols = housing_df.columns[1:-1]\n",
    "print(features_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+--------------------+\n",
      "| ID|   crim|  zn|indus|chas|  nox|   rm| age|   dis|rad|tax|ptratio| black|lstat|medv|            features|\n",
      "+---+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+--------------------+\n",
      "|  1|0.00632|18.0| 2.31|   0|0.538|6.575|65.2|  4.09|  1|296|   15.3| 396.9| 4.98|24.0|[0.00632,18.0,2.3...|\n",
      "|  2|0.02731| 0.0| 7.07|   0|0.469|6.421|78.9|4.9671|  2|242|   17.8| 396.9| 9.14|21.6|[0.02731,0.0,7.07...|\n",
      "|  4|0.03237| 0.0| 2.18|   0|0.458|6.998|45.8|6.0622|  3|222|   18.7|394.63| 2.94|33.4|[0.03237,0.0,2.18...|\n",
      "|  5|0.06905| 0.0| 2.18|   0|0.458|7.147|54.2|6.0622|  3|222|   18.7| 396.9| 5.33|36.2|[0.06905,0.0,2.18...|\n",
      "|  7|0.08829|12.5| 7.87|   0|0.524|6.012|66.6|5.5605|  5|311|   15.2| 395.6|12.43|22.9|[0.08829,12.5,7.8...|\n",
      "+---+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=features_cols, outputCol=\"features\")\n",
    "data_df = assembler.transform(housing_df)\n",
    "data_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------+\n",
      "|features                                                                  |\n",
      "+--------------------------------------------------------------------------+\n",
      "|[0.00632,18.0,2.31,0.0,0.538,6.575,65.2,4.09,1.0,296.0,15.3,396.9,4.98]   |\n",
      "|[0.02731,0.0,7.07,0.0,0.469,6.421,78.9,4.9671,2.0,242.0,17.8,396.9,9.14]  |\n",
      "|[0.03237,0.0,2.18,0.0,0.458,6.998,45.8,6.0622,3.0,222.0,18.7,394.63,2.94] |\n",
      "|[0.06905,0.0,2.18,0.0,0.458,7.147,54.2,6.0622,3.0,222.0,18.7,396.9,5.33]  |\n",
      "|[0.08829,12.5,7.87,0.0,0.524,6.012,66.6,5.5605,5.0,311.0,15.2,395.6,12.43]|\n",
      "+--------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.select(\"features\").show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Normalizzazione\n",
    "\n",
    "E' buona norma portare le features in un range di valori comuni, questo processo può velocizzare anche di molto la fase di addestramento. Facciamolo tramite la normalizzazione che si esegue sottraendo il valore minimo e poi dividendo per la differenza tra valore massimo e valore minimo. Possiamo eseguire la normalizzazione con MLlib usando la classe MinMaxScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+--------------------+--------------------+\n",
      "| ID|   crim|  zn|indus|chas|  nox|   rm| age|   dis|rad|tax|ptratio| black|lstat|medv|            features|     scaled_features|\n",
      "+---+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+--------------------+--------------------+\n",
      "|  1|0.00632|18.0| 2.31|   0|0.538|6.575|65.2|  4.09|  1|296|   15.3| 396.9| 4.98|24.0|[0.00632,18.0,2.3...|[0.0,0.18,0.05814...|\n",
      "|  2|0.02731| 0.0| 7.07|   0|0.469|6.421|78.9|4.9671|  2|242|   17.8| 396.9| 9.14|21.6|[0.02731,0.0,7.07...|[2.85470335157677...|\n",
      "|  4|0.03237| 0.0| 2.18|   0|0.458|6.998|45.8|6.0622|  3|222|   18.7|394.63| 2.94|33.4|[0.03237,0.0,2.18...|[3.54287862356241...|\n",
      "|  5|0.06905| 0.0| 2.18|   0|0.458|7.147|54.2|6.0622|  3|222|   18.7| 396.9| 5.33|36.2|[0.06905,0.0,2.18...|[8.53146933036738...|\n",
      "|  7|0.08829|12.5| 7.87|   0|0.524|6.012|66.6|5.5605|  5|311|   15.2| 395.6|12.43|22.9|[0.08829,12.5,7.8...|[0.00111481674001...|\n",
      "+---+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "scaler_model = scaler.fit(data_df)\n",
    "data_df = scaler_model.transform(data_df)\n",
    "\n",
    "data_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|scaled_features                                                                                                                                                                                                                   |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[0.0,0.18,0.05814814814814815,0.0,0.3148148148148149,0.5836560805577072,0.6297872340425532,0.30899621113279824,0.0,0.20650095602294455,0.31395348837209314,1.0,0.08967991169977926]                                               |\n",
      "|[2.854703351576779E-4,0.0,0.23444444444444446,0.0,0.17283950617283944,0.5538342370255617,0.7755319148936171,0.400544845366205,0.043478260869565216,0.10325047801147227,0.6046511627906979,1.0,0.20447019867549668]                |\n",
      "|[3.542878623562415E-4,0.0,0.05333333333333334,0.0,0.15020576131687244,0.6655693261037956,0.4234042553191489,0.5148475581116202,0.08695652173913043,0.06500956022944551,0.7093023255813954,0.9942297915607524,0.033388520971302425]|\n",
      "|[8.53146933036738E-4,0.0,0.05333333333333334,0.0,0.15020576131687244,0.6944229279628196,0.5127659574468085,0.5148475581116202,0.08695652173913043,0.06500956022944551,0.7093023255813954,1.0,0.09933774834437085]                 |\n",
      "|[0.0011148167400130942,0.125,0.26407407407407407,0.0,0.2860082304526749,0.4746320681642137,0.6446808510638298,0.4624818645819199,0.17391304347826086,0.23518164435946462,0.3023255813953488,0.9966954753431623,0.2952538631346578]|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.select(\"scaled_features\").show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creazione modello di regressione lineare\n",
    "\n",
    "Prossimo passo, dividere il DataFrame con le features preprocessate in due DataFrame, uno per l'addestramento e uno per il testing del modello, possiamo farlo utilizzando il metodo randomSplit all'interno della quale dobbiamo passare una lista con la percentuale di osservazioni da assegnare ad ognuno dei DataFrame.\n",
    "Nel nostro caso assegnamo il 70% degli esempi al set di addestramento e il 30% al set di test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = data_df.randomSplit([0.7, 0.3], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"scaled_features\", labelCol=\"medv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valutiamo il modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.regression.LinearRegressionSummary"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation = model.evaluate(test_df)\n",
    "type(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSE - Mean Squared Error (Errore quadratico assoluto)\n",
    "\n",
    "L'errore quadratico medio consiste nella media della somma degli errori al quadrato.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.444470980244144"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation.meanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSE - Root Mean Squared Error (Radice dell'errore quadratico medio)\n",
    "\n",
    "Il RMSE è la radice dell'errore quadratico medio, questa metrica indica mediamente di quanto il nostro modello si è sbagliato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.517650857044521"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il modello  sbaglia mediamente di 55000 dollari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAE - Mean Absolute Error (Errore medio assoluto)\n",
    "\n",
    "L'errore medio assoluto consiste nella media della somma del valore assoluto degli errori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7370129398441114"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation.meanAbsoluteError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il modello sbaglia mediamente di 37000 dollari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R2 - Coefficient of determination (Coefficiente di Determinazione)\n",
    "\n",
    "In pratica R2 (pronuciato R Squared) è una versione standardizzata del MSE che torna un punteggio compreso tra 0 e 1 per il train set, mentre per il test set può assumere anche valori negativi. Essendo una funzione ma di scoring, un suo valore maggiore indica una qualità migliore del modello, il suo valore può essere così interpretato:\n",
    "\n",
    "    R2_score < 0.3 il modello è inutile.\n",
    "    0.3 < R2_score < 0.5 il modello è scarso.\n",
    "    0.5 < R2_score < 0.7 il modello è discreto.\n",
    "    0.7 < R2_score < 0.9 il modello è buono.\n",
    "    0.9 < R2_score < 1 il modello è ottimo.\n",
    "    R2_score = 1 molto probabilmente c'è un errore nel modello.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6805998721105677"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation.r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testiamo il Modello\n",
    "\n",
    "Ora che abbiamo un modello addestrato e funzionante testiamolo su nuovi dati, mettiamo caso che un'agenzia immobiliare ti abbia mandato un file CSV con le proprietà di 10 abitazioni, per la quale tu devi stimare il prezzo usando il modello che hai addestrato. Iniziamo scaricando il file CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+-----+----+-----+-----+-----+------+---+---+-------+------+-----+\n",
      "| ID|   crim|  zn|indus|chas|  nox|   rm|  age|   dis|rad|tax|ptratio| black|lstat|\n",
      "+---+-------+----+-----+----+-----+-----+-----+------+---+---+-------+------+-----+\n",
      "|  3|0.02729| 0.0| 7.07|   0|0.469|7.185| 61.1|4.9671|  2|242|   17.8|392.83| 4.03|\n",
      "|  6|0.02985| 0.0| 2.18|   0|0.458| 6.43| 58.7|6.0622|  3|222|   18.7|394.12| 5.21|\n",
      "|  8|0.14455|12.5| 7.87|   0|0.524|6.172| 96.1|5.9505|  5|311|   15.2| 396.9|19.15|\n",
      "|  9|0.21124|12.5| 7.87|   0|0.524|5.631|100.0|6.0821|  5|311|   15.2|386.63|29.93|\n",
      "| 10|0.17004|12.5| 7.87|   0|0.524|6.004| 85.9|6.5921|  5|311|   15.2|386.71| 17.1|\n",
      "+---+-------+----+-----+----+-----+-----+-----+------+---+---+-------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "houses_df = spark.read.csv(\"./data/houses_test.csv\", inferSchema=True, header=True)\n",
    "houses_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+-----+----+-----+-----+-----+------+---+---+-------+------+-----+--------------------+\n",
      "| ID|   crim|  zn|indus|chas|  nox|   rm|  age|   dis|rad|tax|ptratio| black|lstat|            features|\n",
      "+---+-------+----+-----+----+-----+-----+-----+------+---+---+-------+------+-----+--------------------+\n",
      "|  3|0.02729| 0.0| 7.07|   0|0.469|7.185| 61.1|4.9671|  2|242|   17.8|392.83| 4.03|[0.02729,0.0,7.07...|\n",
      "|  6|0.02985| 0.0| 2.18|   0|0.458| 6.43| 58.7|6.0622|  3|222|   18.7|394.12| 5.21|[0.02985,0.0,2.18...|\n",
      "|  8|0.14455|12.5| 7.87|   0|0.524|6.172| 96.1|5.9505|  5|311|   15.2| 396.9|19.15|[0.14455,12.5,7.8...|\n",
      "|  9|0.21124|12.5| 7.87|   0|0.524|5.631|100.0|6.0821|  5|311|   15.2|386.63|29.93|[0.21124,12.5,7.8...|\n",
      "| 10|0.17004|12.5| 7.87|   0|0.524|6.004| 85.9|6.5921|  5|311|   15.2|386.71| 17.1|[0.17004,12.5,7.8...|\n",
      "+---+-------+----+-----+----+-----+-----+-----+------+---+---+-------+------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_df = assembler.transform(houses_df)\n",
    "input_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+-----+----+-----+-----+-----+------+---+---+-------+------+-----+--------------------+--------------------+\n",
      "| ID|   crim|  zn|indus|chas|  nox|   rm|  age|   dis|rad|tax|ptratio| black|lstat|            features|     scaled_features|\n",
      "+---+-------+----+-----+----+-----+-----+-----+------+---+---+-------+------+-----+--------------------+--------------------+\n",
      "|  3|0.02729| 0.0| 7.07|   0|0.469|7.185| 61.1|4.9671|  2|242|   17.8|392.83| 4.03|[0.02729,0.0,7.07...|[2.85198329121319...|\n",
      "|  6|0.02985| 0.0| 2.18|   0|0.458| 6.43| 58.7|6.0622|  3|222|   18.7|394.12| 5.21|[0.02985,0.0,2.18...|[3.20015101775138...|\n",
      "|  8|0.14455|12.5| 7.87|   0|0.524|6.172| 96.1|5.9505|  5|311|   15.2| 396.9|19.15|[0.14455,12.5,7.8...|[0.00187996972028...|\n",
      "|  9|0.21124|12.5| 7.87|   0|0.524|5.631|100.0|6.0821|  5|311|   15.2|386.63|29.93|[0.21124,12.5,7.8...|[0.00278697384852...|\n",
      "| 10|0.17004|12.5| 7.87|   0|0.524|6.004| 85.9|6.5921|  5|311|   15.2|386.71| 17.1|[0.17004,12.5,7.8...|[0.00222664141362...|\n",
      "+---+-------+----+-----+----+-----+-----+-----+------+---+---+-------+------+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_df = scaler_model.transform(input_df)\n",
    "input_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = model.transform(input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+-----+----+-----+-----+-----+------+---+---+-------+------+-----+--------------------+--------------------+------------------+\n",
      "| ID|   crim|  zn|indus|chas|  nox|   rm|  age|   dis|rad|tax|ptratio| black|lstat|            features|     scaled_features|        prediction|\n",
      "+---+-------+----+-----+----+-----+-----+-----+------+---+---+-------+------+-----+--------------------+--------------------+------------------+\n",
      "|  3|0.02729| 0.0| 7.07|   0|0.469|7.185| 61.1|4.9671|  2|242|   17.8|392.83| 4.03|[0.02729,0.0,7.07...|[2.85198329121319...|29.967527267571267|\n",
      "|  6|0.02985| 0.0| 2.18|   0|0.458| 6.43| 58.7|6.0622|  3|222|   18.7|394.12| 5.21|[0.02985,0.0,2.18...|[3.20015101775138...|24.811553114211552|\n",
      "|  8|0.14455|12.5| 7.87|   0|0.524|6.172| 96.1|5.9505|  5|311|   15.2| 396.9|19.15|[0.14455,12.5,7.8...|[0.00187996972028...|17.602371609554382|\n",
      "|  9|0.21124|12.5| 7.87|   0|0.524|5.631|100.0|6.0821|  5|311|   15.2|386.63|29.93|[0.21124,12.5,7.8...|[0.00278697384852...| 9.193577569780444|\n",
      "| 10|0.17004|12.5| 7.87|   0|0.524|6.004| 85.9|6.5921|  5|311|   15.2|386.71| 17.1|[0.17004,12.5,7.8...|[0.00222664141362...|17.150020489912727|\n",
      "+---+-------+----+-----+----+-----+-----+-----+------+---+---+-------+------+-----+--------------------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "pred_df = pred_df.drop(\"features\") \\\n",
    "                 .drop(\"scaled_features\") \\\n",
    "                 .withColumn(\"estimated_price\", round(pred_df[\"prediction\"]*10000, 2)) \\\n",
    "                 .drop(\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
